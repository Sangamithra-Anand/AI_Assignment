# ğŸ· PCA + K-Means Clustering Project  
Dimensionality Reduction & Clustering on the Wine Dataset

This project demonstrates a complete Machine Learning workflow using:

- **Exploratory Data Analysis (EDA)**
- **Data Preprocessing**
- **Principal Component Analysis (PCA)**
- **K-Means Clustering (Before & After PCA)**
- **Cluster Quality Evaluation**
- **Performance Comparison Between Original and PCA Data**

The project is fully automated through a single pipeline:

```bash
python src/main.py
```

---

## ğŸ“Œ 1. Project Overview

The goal of this project is to understand how **dimensionality reduction** using PCA affects clustering performance.  
We apply **K-Means** clustering twice:

1. **On Original Scaled Features**  
2. **On PCA-Transformed Features**

We then compare both results using:

- **Silhouette Score**
- **Daviesâ€“Bouldin Index**

A final comparison report is generated automatically.

---

## ğŸ› ï¸ 2. Technologies Used

- Python  
- NumPy  
- Pandas  
- Matplotlib  
- Seaborn  
- Scikit-Learn  

---

## ğŸ“‚ 3. Project Folder Structure

```
pca_clustering_project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          â† Manual
â”‚   â”‚   â””â”€â”€ wine.csv
â”‚   â””â”€â”€ processed/                    â† Auto
â”‚       â”œâ”€â”€ scaled_wine.csv
â”‚       â””â”€â”€ pca_transformed.csv
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ eda_plots/                    â† Auto
â”‚   â”œâ”€â”€ pca_plots/                    â† Auto
â”‚   â”œâ”€â”€ clustering_plots/             â† Auto
â”‚   â”œâ”€â”€ visuals/                      â† Auto
â”‚   â””â”€â”€ reports/                      â† Auto
â”‚       â”œâ”€â”€ pca_summary.json
â”‚       â”œâ”€â”€ clustering_original_scores.json
â”‚       â”œâ”€â”€ clustering_pca_scores.json
â”‚       â””â”€â”€ comparison_report.md
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ load_data.py
â”‚   â”œâ”€â”€ eda.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ pca_model.py
â”‚   â”œâ”€â”€ clustering_original.py
â”‚   â”œâ”€â”€ clustering_pca.py
â”‚   â”œâ”€â”€ compare_results.py
â”‚   â”œâ”€â”€ visualize.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## â–¶ï¸ 4. How to Run the Project

### **1. Install requirements**
```bash
pip install -r requirements.txt
```

### **2. Run the full pipeline**
```bash
python src/main.py
```

Everything will be generated automatically inside the **outputs/** folder.

---

## ğŸ” 5. Workflow Summary

### **Step 1 â€” EDA**
- Histograms  
- Boxplots  
- Correlation Heatmap  

Saved at: `outputs/eda_plots/`

### **Step 2 â€” Preprocessing**
- Keep numeric columns  
- Fill missing values  
- Standard scaling  

Saved at: `data/processed/scaled_wine.csv`

### **Step 3 â€” PCA**
- Scree Plot  
- Cumulative Variance Plot  
- PCA-transformed dataset  

Saved at:  
- `outputs/pca_plots/`  
- `data/processed/pca_transformed.csv`

### **Step 4 â€” K-Means Clustering**
Clustering performed on:

- **Original Scaled Data**
- **PCA-Transformed Data**

Saved at:  
- `outputs/clustering_plots/`  
- `outputs/reports/`

### **Step 5 â€” Comparison Report**
Automatically generated evaluation showing which approach performed better.

Saved at:  
`outputs/reports/comparison_report.md`

---

## ğŸ§  6. Concepts Used

### **ğŸ“Œ Principal Component Analysis (PCA)**
- Reduces dimensionality  
- Removes correlation between features  
- Helps visualize high-dimensional data  
- Often improves clustering  

### **ğŸ“Œ K-Means Clustering**
- Unsupervised learning  
- Groups similar data points  
- Improved by scaling  
- Can perform better on PCA-transformed data  

### **ğŸ“Œ Evaluation Metrics**

| Metric | Meaning | Good Value |
|--------|---------|------------|
| **Silhouette Score** | Cluster separation quality | Closer to +1 |
| **Daviesâ€“Bouldin Index** | Cluster compactness | Lower is better |

---

## ğŸ“Š 7. Key Outputs

| Output | Location | Description |
|--------|----------|------------|
| Histograms | `outputs/eda_plots/` | Distribution of each feature |
| Scree Plot | `outputs/pca_plots/` | Variance explained by components |
| PCA Dataset | `data/processed/pca_transformed.csv` | Reduced features |
| Cluster Plots | `outputs/clustering_plots/` | Visual cluster separation |
| Comparison Report | `outputs/reports/comparison_report.md` | Final evaluation |

---

## ğŸ 8. Conclusion & Insights

- PCA helps simplify the dataset while keeping important information.  
- K-Means sometimes performs better on PCA-transformed data because:
  - Noise is reduced  
  - Highly correlated features are removed  
  - Lower dimensions â†’ easier clustering  
- The comparison metrics reveal whether PCA improved clustering performance for this dataset.

This project demonstrates a **complete, end-to-end unsupervised learning workflow** suitable for real-world machine learning pipelines.

---

## ğŸ“ Contact

Feel free to reach out if you have any questions or suggestions about this project!

